Quash Full Stack Assignment 
Conversational Browser Control Agent 
Assignment Overview 

Build an AI-powered browser automation agent that controls a real browser through 
natural-language conversation. The system should showcase strong backend design, reliable 
browser control, conversational UX, and clean AI orchestration.​
Progress must be conveyed via live action traces, state updates, and extracted data in the chat. 

 

Objective 
Create an intelligent system that can: 

●​ Understand user intent from free-form text.​
 

●​ Control a real browser using a mainstream automation stack.​
 

●​ Stream structured progress updates (actions taken, targets selected, values 
extracted) in a chat UI.​
 

●​ Keep a modular, extensible architecture suitable for adding future workflows (search, 
form-fill, comparison, booking, etc.).​
 

 

 

 

 

 



Core Technical Requirements 
1) Natural Language Understanding (NLU) 

The agent must interpret commands like: 

●​ “Book Friday night tickets for an ABC movie.”​
 

●​ “Compare the first three laptops under ₹60,000 on Flipkart.”​
 

●​ “Fill out the signup form on this URL and submit.”​
 

●​ “Find top 3 pizza places near Indiranagar with 4★+ ratings.”​
 

Expectations: 

●​ Extract intent, parameters, targets, filters, and constraints.​
 

 

2) Browser Automation Engine (general) 

Use a well-known automation approach (choose one): 

●​ Playwright or Puppeteer​
 

●​ Selenium / WebDriver BiDi​
 

●​ Chrome DevTools Protocol (CDP) (see optional Electron example below)​
 

Required capabilities: 

●​ Navigation with robust waits (network idle, DOM ready, or specific selector).​
 

●​ Element interaction: click, type, select, scroll; prefer stable selectors (data-testid/role) 
and fallbacks when necessary.​
 

●​ Extraction: query page content and return structured JSON (e.g., list of items with 
name, price, rating, URL).​
 

●​ State reporting (instead of screenshots):​
 



○​ Action trace: navigate, wait_for, query, click, type, extract​
 

○​ Element target summary (selector + short description)​
 

○​ Result snippets (e.g., “Found 17 results; top 3: …”)​
 

○​ Key state signals (URL/title change, modal detected, list count changed)​
 

 

3) Conversational Interface (Chat UX) 

Provide a minimal web UI that: 

●​ Shows user ↔ agent messages as chat bubbles.​
 

●​ Streams live status updates and action cards (e.g., “typed into #email”, “clicked [Add 
to cart]”, “extracted 3 results”).​
 

●​ Clearly surfaces blocked states and helpful suggestions (“selector not found; refine 
your filter?”).​
 

(No images. The conversation should feel “live” through structured traces and concise DOM 
summaries.) 

 

4) AI-Powered Reasoning & Generation 

Leverage an LLM (OpenAI/Claude/Gemini or open-source) to: 

●​ Interpret instructions and plan steps (small explicit planner is fine).​
 

●​ Generate text inputs when needed (search queries, form content, summaries).​
 

●​ Adapt to user feedback mid-run.​
 

●​ Keep prompts, parsers, and chain logic modular & versioned.​
 

 

 



5) Robust, Extensible Architecture 

Separate concerns cleanly: 

●​ Conversation Layer – turn handling, context memory, clarifications.​
 

●​ Planner/Orchestrator – intent → plan → action loop.​
 

●​ Browser Layer – navigation, interaction, extraction, resilience.​
 

●​ AI Layer – model calls, prompt templates, parsing.​
 

●​ UI Layer – chat, action cards, streaming.​
 

Design so adding a new “capability” (e.g., compare items across two sites) does not require 
rewiring the core. 

 

Example User Journeys (choose any one end-to-end) 
A. Product search & comparison​
 User: “Find MacBook Air 13-inch under ₹1,00,000; give me top 3 with rating and links.”​
 Agent (streaming): 

●​ navigate → store URL → wait_for: networkIdle​
 

●​ type → search box: “MacBook Air 13 inch”​
 

●​ apply_filter → price ≤ 100000​
 

●​ extract → first 3 results {name, price, rating, url}​
 Agent: “Here are the top 3 results … Want to filter by color or seller?”​
 

B. Form fill & submit​
 User: “Open this signup page and register with a temporary email.”​
 Agent: navigates, fills fields with generated values, handles validation errors, submits, returns 
{status, message, redirect_url}. 

C. Local discovery​
 User: “Show top pizza places near Indiranagar with ratings and delivery availability.”​
 Agent: asks preferred site, searches, extracts structured top 3, returns concise summary. 



 

Optional Implementation Example (CDP + Electron) 
Candidates may implement the browser layer by spinning up Electron and attaching a CDP 
session to the BrowserWindow: 

●​ Use CDP domains (Page, Runtime, DOM, Network) to navigate, evaluate in page, and 
extract JSON.​
 

●​ Stream action events over WebSocket to the renderer for live chat updates.​
 This is one acceptable approach—not required.​
 

 

Technical Implementation Guidelines 
Backend 

●​ Python (FastAPI).​
 

●​ Modules for NLU, planning, browser control, extraction, message streaming.​
 

●​ WebSockets (preferred) or SSE for real-time action events.​
 

●​ Retries with backoff; element re-query strategies; navigation timeouts.​
 

●​ Structured JSON logging with redaction.​
 

Frontend 

●​ Minimal React/Next.js chat UI (or Electron renderer):​
 

○​ Input + scrolling chat history.​
 

○​ Action cards with action name, selector/target summary, outcome payload.​
 

○​ Status chips: navigating, typing, extracting, blocked, completed.​
 

 



Deliverables 
1.​ Codebase (GitHub or zip) with README.md (install/run), env sample, and scripts.​

 
2.​ Demo Video (3–5 min) showing natural-language input → live action stream → final 

structured result.​
 

3.​ Technical Write-Up (~1 page): architecture, trade-offs, failure handling, next steps.​
 

4.​ (Optional) Reasoning/plan logs (redacted).​
 

 

Evaluation Criteria 
Category What we’re scoring 

Architecture Clean layering; planner–executor separation 

Backend Reasoning Clear intent → plan → action loop 

Automation Reliability Stable waits/selectors, retries, recovery 

Conversation Design Clarifications, multi-turn memory, helpful tone 

UI/UX Real-time action feed that feels “live” (no images) 

AI Integration Planning quality; useful generated inputs 

Error Handling Graceful fallbacks and user prompts on 
blockages 

Docs Setup clarity and depth of explanation 

Time expectation: ~6–8 hours. Emphasize sound judgment and working depth over polish. 

 

 

 



Bonus Points 
●​ WebSocket streaming with granular action events.​

 
●​ Memory of user preferences across tasks.​

 
●​ Multi-site comparison flows.​

 
●​ Provider abstraction for LLMs and pluggable planners.​

 
●​ Dockerized dev stack.​

 
●​ Deterministic e2e test(s) for one workflow.​